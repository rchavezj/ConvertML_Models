{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from miscc.utils import mkdir_p\n",
    "from miscc.utils import build_super_images\n",
    "from miscc.losses import sent_loss, words_loss\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "\n",
    "from datasets import TextDataset\n",
    "from datasets import prepare_data\n",
    "\n",
    "from model import RNN_ENCODER, CNN_ENCODER\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    # build model ############################################################\n",
    "    text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "    print(\"cfg.TEXT.EMBEDDING_DIM\", cfg.TEXT.EMBEDDING_DIM) #256\n",
    "#     image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n",
    "    labels = Variable(torch.LongTensor(range(batch_size)))\n",
    "    start_epoch = 0\n",
    "    if cfg.TRAIN.NET_E != '':\n",
    "        state_dict = torch.load(cfg.TRAIN.NET_E)\n",
    "        text_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', cfg.TRAIN.NET_E)\n",
    "        #\n",
    "        name = cfg.TRAIN.NET_E.replace('text_encoder')\n",
    "        state_dict = torch.load(name)\n",
    "#         image_encoder.load_state_dict(state_dict)\n",
    "        print('Load ', name)\n",
    "\n",
    "        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n",
    "        iend = cfg.TRAIN.NET_E.rfind('.')\n",
    "        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n",
    "        start_epoch = int(start_epoch) + 1\n",
    "        print('start_epoch', start_epoch)\n",
    "    if cfg.CUDA:\n",
    "        text_encoder = text_encoder\n",
    "#         image_encoder = image_encoder\n",
    "        labels = labels\n",
    "\n",
    "    return text_encoder, labels, start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B_VALIDATION': False,\n",
      " 'CONFIG_NAME': 'DAMSM',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'coco',\n",
      " 'DATA_DIR': '../data/coco',\n",
      " 'GAN': {'B_ATTENTION': True,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 64,\n",
      "         'GF_DIM': 128,\n",
      "         'R_NUM': 2,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': 0,\n",
      " 'RNN_TYPE': 'LSTM',\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 15},\n",
      " 'TRAIN': {'BATCH_SIZE': 48,\n",
      "           'B_NET_D': True,\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'ENCODER_LR': 0.002,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'NET_E': '',\n",
      "           'NET_G': '',\n",
      "           'RNN_GRAD_CLIP': 0.25,\n",
      "           'SMOOTH': {'GAMMA1': 4.0,\n",
      "                      'GAMMA2': 5.0,\n",
      "                      'GAMMA3': 10.0,\n",
      "                      'LAMBDA': 1.0},\n",
      "           'SNAPSHOT_INTERVAL': 5},\n",
      " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
      " 'WORKERS': 1}\n"
     ]
    }
   ],
   "source": [
    "#Configurations\n",
    "cfg_from_file(\"cfg/DAMSM/coco.yml\")\n",
    "import pprint\n",
    "pprint.pprint(cfg)\n",
    "random.seed(random.randint(1, 10000))\n",
    "np.random.seed(random.randint(1, 10000))\n",
    "torch.manual_seed(random.randint(1, 10000))\n",
    "if cfg.CUDA:\n",
    "    torch.cuda.manual_seed_all(random.randint(1, 10000))\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = '../output/%s_%s_%s' % \\\n",
    "    (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "model_dir = os.path.join(output_dir, 'Model')\n",
    "image_dir = os.path.join(output_dir, 'Image')\n",
    "mkdir_p(model_dir)\n",
    "mkdir_p(image_dir)\n",
    "# Debug\n",
    "torch.cuda.device(cfg.GPU_ID)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load filenames from: ../data/coco/train/filenames.pickle (82783)\n",
      "Load filenames from: ../data/coco/test/filenames.pickle (40470)\n",
      "Load from:  ../data/coco/captions.pickle\n",
      "=====================================================\n",
      "cfg.DATA_DIR:  ../data/coco\n",
      "cfg.TREE.BASE_SIZE:  299\n",
      "image_transform:  <torchvision.transforms.Compose object at 0x1c14804128>\n",
      "cfg.TEXT.EMBEDDING_DIM 256\n",
      "=====================================================\n",
      "dataset.n_words: 27297  dataset.embeddings_num:  5\n",
      "Load filenames from: ../data/coco/train/filenames.pickle (82783)\n",
      "Load filenames from: ../data/coco/test/filenames.pickle (40470)\n",
      "Load from:  ../data/coco/captions.pickle\n"
     ]
    }
   ],
   "source": [
    "# cfg.TREE.BASE_SIZE = 299\n",
    "imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n",
    "batch_size = cfg.TRAIN.BATCH_SIZE\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Scale(int(imsize * 76 / 64)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "dataset = TextDataset(\"../data/coco\", 'train',\n",
    "                      base_size=299,\n",
    "                      transform=image_transform)\n",
    "print(\"=====================================================\")\n",
    "print(\"cfg.DATA_DIR: \", cfg.DATA_DIR)\n",
    "print(\"cfg.TREE.BASE_SIZE: \", cfg.TREE.BASE_SIZE)\n",
    "print(\"image_transform: \", image_transform)\n",
    "print(\"cfg.TEXT.EMBEDDING_DIM\", cfg.TEXT.EMBEDDING_DIM)\n",
    "print(\"=====================================================\")\n",
    "print(\"dataset.n_words:\", dataset.n_words, \" dataset.embeddings_num: \", dataset.embeddings_num)\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))\n",
    "\n",
    "# # validation data #\n",
    "dataset_val = TextDataset(\"../data/coco\", 'test',\n",
    "                          base_size=cfg.TREE.BASE_SIZE,\n",
    "                          transform=image_transform)\n",
    "dataloader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=batch_size, drop_last=True,\n",
    "    shuffle=True, num_workers=int(cfg.WORKERS))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "split_dir, bshuffle = 'train', True\n",
    "if not cfg.TRAIN.FLAG:\n",
    "    # bshuffle = False\n",
    "    split_dir = 'test'\n",
    "dataset = TextDataset(cfg.DATA_DIR, 'train',base_size=cfg.TREE.BASE_SIZE,transform=image_transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.TRAIN.BATCH_SIZE,drop_last=True, shuffle=bshuffle, num_workers=int(cfg.WORKERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.TextDataset at 0x1c1724f908>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.TEXT.EMBEDDING_DIM 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/anaconda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "text_encoder, labels, start_epoch = build_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_ENCODER(\n",
       "  (encoder): Embedding(27297, 300)\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (rnn): LSTM(300, 128, batch_first=True, dropout=0.5, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.train()\n",
    "text_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 \n",
      "cap_lens\n",
      " tensor([ 15,  15,  15,  15,  14,  13,  13,  12,  11,  11,  11,  11,\n",
      "         11,  11,  11,  11,  11,  10,  10,  10,  10,  10,  10,  10,\n",
      "         10,  10,  10,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "          9,   9,   9,   9,   8,   8,   8,   8,   8,   8,   8,   8]) \n",
      "\n",
      "step 1 \n",
      "cap_lens\n",
      " tensor([ 15,  15,  15,  15,  13,  13,  13,  13,  13,  12,  12,  12,\n",
      "         12,  12,  12,  11,  11,  11,  11,  11,  11,  10,  10,  10,\n",
      "         10,  10,  10,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
      "          9,   9,   9,   8,   8,   8,   8,   8,   8,   8,   8,   8]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(dataloader, 0):\n",
    "    imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n",
    "    hidden = text_encoder.init_hidden(batch_size)\n",
    "    print(\"step\", step, \"\\ncap_lens\\n\", cap_lens, \"\\n\") \n",
    "    if step == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 15,  15,  15,  15,  13,  13,  13,  13,  13,  12,  12,  12,\n",
       "         12,  12,  12,  11,  11,  11,  11,  11,  11,  10,  10,  10,\n",
       "         10,  10,  10,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
       "          9,   9,   9,   8,   8,   8,   8,   8,   8,   8,   8,   8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]]),\n",
       " tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          ...,\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Keras version 2.1.6 detected. Last version known to be fully compatible of Keras is 2.1.3 .\n",
      "WARNING:root:TensorFlow version 1.8.0 detected. Last version known to be fully compatible is 1.5.0 .\n"
     ]
    }
   ],
   "source": [
    "# source coreml/bin/activate\n",
    "import coremltools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
